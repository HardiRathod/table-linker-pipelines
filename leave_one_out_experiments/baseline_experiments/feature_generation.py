# Purpose
# This script generates pseudo ground truth based features for all the leave one out experiments. 
# TO set the feature generation process, select the test dataset and train/dev datasets.
# For accessing data, set AWS s3 Access ID and Key and data will be automatically downloaded during the run.
# Also need to set the pseudo_ground_truth best model and min_max_scaler - These are generated by running the pgt pipeline over each set. 


###Importing all the libraries
import glob
import boto3
import time
import os
import pandas as pd
import sklearn.metrics
from sklearn.preprocessing import MinMaxScaler
import pickle
from argparse import ArgumentParser, Namespace
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import Dataset, DataLoader
from torch.optim import Adam
from itertools import chain
import copy
import shutil
import pickle

# Defining connection to s3
ACCESS_ID = '<YOUR ID>'
ACCESS_KEY = '<YOUR KEY>'
s3 = boto3.client('s3', aws_access_key_id=ACCESS_ID,
                  aws_secret_access_key=ACCESS_KEY, use_ssl=True)
bucket = 'table-linker-datasets'
###Defining all the results path and version controls
test_dataset = 'biodiv'
experiment_name = f"Experiment_test_{test_dataset}"
experiment_store_path = f"Experiments/{experiment_name}"
train_datasets_list = ['2t', 'limaye', 't2dv2', 'biotab', 'semtab']

final_score_column = 'siamese_prediction'
threshold = final_score_column + ":median"
pgt_saved_model_path = 'Experiments/Experiment_test_biodiv/pgt_models/top1_0.6626443716600586_epoch_1_loss_0.2317839115858078_batch_size_32_learning_rate_0.0001.pth'
pgt_saved_scaler_path = 'Experiments/Experiment_test_biodiv/training_data/pseudo_pipeline_normalization_factor.pkl'

s3.download_file('table-linker-datasets',
                 pgt_saved_model_path,
                 '/tmp/pgt_saved_model.pth')
s3.download_file('table-linker-datasets',
                 pgt_saved_scaler_path,
                 '/tmp/normalization_factor.pkl')

s3_1 = boto3.resource("s3", aws_access_key_id=ACCESS_ID,
                      aws_secret_access_key=ACCESS_KEY)
s3_bucket = s3_1.Bucket(bucket)
pseudo_gt_model = '/tmp/pgt_saved_model.pth'
pseudo_gt_min_max_scaler_path = '/tmp/normalization_factor.pkl'
pseudo_gt_features = ["monge_elkan", "monge_elkan_aliases", "jaro_winkler", "levenshtein", "singleton", "pgr_rts",
                      "context_score", "smc_class_score", "smc_property_score"]

### Functions needed to train the files.
def read_file(key):
    resp = s3.get_object(Bucket=bucket, Key=key)
    try:
        df = pd.read_csv(resp['Body'], sep=',')
    except pd.errors.EmptyDataError:
        df = ''
        print('Empty csv file!')
    return df


def save_file(key, content):
    s3_res = boto3.resource('s3')

    object = s3_res.Object(bucket, key)
    result = object.put(Body=content)



def features_1(file, output_path, pseudo_gt_model, pseudo_gt_min_max_scaler_path):
    # file_list = glob.glob(candidate_dir + '/*.csv')
    # for i, file in tqdm(enumerate(file_list)):
    print(file)

    if os.path.getsize(file) > 0:
        filename = file.split('/')[-1]
        output_file = f"{output_path}/{filename}"
        # context_file = f"{context_path}/{filename[:-4]}_context.jl"
        # class_count_file = f"{class_count_dir}/{filename[:-4]}_class_count.tsv"
        # property_count_file = f"{property_count_dir}/{filename[:-4]}_prop_count.tsv"
        feature_str = ",".join(pseudo_gt_features)
        # context_property_file = f"{context_property_path}/{filename[:-4]}_context_properties.csv"
        '''
        resp = s3.get_object(Bucket = 'table-linker-datasets', Key = file)
        '''
        command = "tl" + " predict-using-model " + file + " -o pseudo_gt_prediction \
                --features " + feature_str + " \
                --ranking-model " + pseudo_gt_model + " \
                --ignore-column ignore_candidate \
                --normalization-factor " + pseudo_gt_min_max_scaler_path + " > /tmp/file_x.csv"
        command_2 = "tl  create-pseudo-gt -o pseudo_gt /tmp/file_x.csv\
                --column-thresholds pseudo_gt_prediction:mean \
                --filter smc_class_score:0 > /tmp/file.csv"
        print(command)
        os.system(command)
        os.system(command_2)

        # result.to_csv(output_path)


def features_2(file, output_path, context_file, context_property_file, filename):
    if os.path.getsize(file) > 0:
        # filename = file.split('/')[-1]
        output_file = f"{output_path}/{filename}"
        # context_file = f"{context_path}/{filename[:-4]}_context.jl"
        # context_property_file = f"{context_property_path}/{filename[:-4]}_context_properties.csv"
        command = "tl context-match " + file + " --debug --context-file " + context_file + " -o context_score_3 " + " \
             --similarity-string-threshold 0.85 --similarity-quantity-threshold 0.85 \
             --use-relevant-properties --context-properties-path " + context_property_file + " > /tmp/file2.csv"
        print(command)
        os.system(command)


def features_3(file, output_path, embedding_file, class_count_file, property_count_file, filename):
    if os.path.getsize(file) > 0:
        # filename = file.split('/')[-1]
        output_file = f"{output_path}/{filename}"

        command = "tl mosaic-features -c kg_labels --num-char --num-tokens " + file + " \
        / score-using-embedding \
        --column-vector-strategy centroid-of-lof \
        --lof-strategy pseudo-gt \
        -o pgt_centroid_score \
        --embedding-file " + embedding_file + " \
        / compute-tf-idf  \
        --feature-file " + class_count_file + " \
        --feature-name class_count \
        --singleton-column pseudo_gt \
        -o pgt_class_count_tf_idf_score \
        / compute-tf-idf \
        --feature-file " + property_count_file + " \
        --feature-name property_count \
        --singleton-column pseudo_gt \
        -o pgt_property_count_tf_idf_score > /tmp/file.csv"
        print(command)
        os.system(command)
        print(output_file)
        s3.upload_file('/tmp/file.csv', 'table-linker-datasets', output_file)


# features_1(dev_candidate_path, dev_features1_path, dev_context_path, dev_class_count, dev_prop_count,pseudo_gt_model, pseudo_gt_min_max_scaler_path, dev_context_property_path)
def handler(event, context):
    print("Running feature generation for train data")
    for c_dataset in train_datasets_list:
        set_of_files = [i.key for i in s3_bucket.objects.filter(Prefix=f'Experiments/reduced_train_data/{c_dataset}_data/').all()]
        print(set_of_files)
        for file in set_of_files:
            if not file.endswith('.csv'):
                continue
            filename = file.split('/')[-1]
            header_folder = f'datasets/{c_dataset}_data'
            context_file_path = f'{header_folder}/dev_context/{filename[:-4]}_context.jl'
            context_property_file_path = f'{header_folder}/dev_context_properties/{filename[:-4]}_context_properties.csv'
            class_count_file_path = f'{header_folder}/dev_class_count/{filename[:-4]}_class_count.tsv'
            property_count_file_path = f'{header_folder}/dev_prop_count/{filename[:-4]}_prop_count.tsv'
            graph_embedding_file_path = f'{header_folder}/dev_graph_embedding/{filename[:-4]}_graph_embedding_complex.tsv'
            s3.download_file('table-linker-datasets', file, '/tmp/base_copy.csv')
            s3.download_file('table-linker-datasets', context_file_path,
                             '/tmp/base_context.jl')
            s3.download_file('table-linker-datasets', class_count_file_path,
                             '/tmp/base_class_count.tsv')
            s3.download_file('table-linker-datasets', property_count_file_path,
                             '/tmp/base_prop_count.tsv')
            s3.download_file('table-linker-datasets', graph_embedding_file_path,
                             '/tmp/base_graph_embedding_complex.tsv')
            s3.download_file('table-linker-datasets', context_property_file_path,
                             '/tmp/base_context_properties.csv')
            # filename = '00UQHNO3'
            features_1('/tmp/base_copy.csv', 'datasets', pseudo_gt_model, pseudo_gt_min_max_scaler_path)
            features_2('/tmp/file.csv', 'datasets', '/tmp/base_context.jl', '/tmp/base_context_properties.csv', filename)
            features_3('/tmp/file2.csv', f'Experiments/{experiment_name}/reduced_train_data/{c_dataset}_data',
                       '/tmp/base_graph_embedding_complex.tsv', '/tmp/base_class_count.tsv',
                       '/tmp/base_prop_count.tsv', filename)
    print("Running for dev data")
    for c_dataset in train_datasets_list:
        set_of_files = [i.key for i in s3_bucket.objects.filter(Prefix=f'Experiments/reduced_train_data/{c_dataset}_data/').all()]
        print(set_of_files)
        for file in set_of_files:
            if not file.endswith('.csv'):
                continue
            filename = file.split('/')[-1]
            header_folder = f'datasets/{c_dataset}_data'
            context_file_path = f'{header_folder}/dev_context/{filename[:-4]}_context.jl'
            context_property_file_path = f'{header_folder}/dev_context_properties/{filename[:-4]}_context_properties.csv'
            class_count_file_path = f'{header_folder}/dev_class_count/{filename[:-4]}_class_count.tsv'
            property_count_file_path = f'{header_folder}/dev_prop_count/{filename[:-4]}_prop_count.tsv'
            graph_embedding_file_path = f'{header_folder}/dev_graph_embedding/{filename[:-4]}_graph_embedding_complex.tsv'
            s3.download_file('table-linker-datasets', file, '/tmp/base_copy.csv')
            s3.download_file('table-linker-datasets', context_file_path,
                             '/tmp/base_context.jl')
            s3.download_file('table-linker-datasets', class_count_file_path,
                             '/tmp/base_class_count.tsv')
            s3.download_file('table-linker-datasets', property_count_file_path,
                             '/tmp/base_prop_count.tsv')
            s3.download_file('table-linker-datasets', graph_embedding_file_path,
                             '/tmp/base_graph_embedding_complex.tsv')
            s3.download_file('table-linker-datasets', context_property_file_path,
                             '/tmp/base_context_properties.csv')
            # filename = '00UQHNO3'
            features_1('/tmp/base_copy.csv', 'datasets', pseudo_gt_model, pseudo_gt_min_max_scaler_path)
            features_2('/tmp/file.csv', 'datasets', '/tmp/base_context.jl', '/tmp/base_context_properties.csv', filename)
            features_3('/tmp/file2.csv', f'Experiments/{experiment_name}/reduced_train_data/{c_dataset}_data',
                       '/tmp/base_graph_embedding_complex.tsv', '/tmp/base_class_count.tsv',
                       '/tmp/base_prop_count.tsv', filename)
            
    print("Running for test data")
    set_of_files = [i.key for i in s3_bucket.objects.filter(Prefix=f'datasets/{test_dataset}_data/complete_dataset/').all()]
    print(set_of_files)
    for file in set_of_files:
        if not file.endswith('.csv'):
            continue
        filename = file.split('/')[-1]
        header_folder = f'datasets/{test_dataset}_data'
        context_file_path = f'{header_folder}/dev_context/{filename[:-4]}_context.jl'
        context_property_file_path = f'{header_folder}/dev_context_properties/{filename[:-4]}_context_properties.csv'
        class_count_file_path = f'{header_folder}/dev_class_count/{filename[:-4]}_class_count.tsv'
        property_count_file_path = f'{header_folder}/dev_prop_count/{filename[:-4]}_prop_count.tsv'
        graph_embedding_file_path = f'{header_folder}/dev_graph_embedding/{filename[:-4]}_graph_embedding_complex.tsv'
        s3.download_file('table-linker-datasets', file, '/tmp/base_copy.csv')
        s3.download_file('table-linker-datasets', context_file_path,
                         '/tmp/base_context.jl')
        s3.download_file('table-linker-datasets', class_count_file_path,
                         '/tmp/base_class_count.tsv')
        s3.download_file('table-linker-datasets', property_count_file_path,
                         '/tmp/base_prop_count.tsv')
        s3.download_file('table-linker-datasets', graph_embedding_file_path,
                         '/tmp/base_graph_embedding_complex.tsv')
        s3.download_file('table-linker-datasets', context_property_file_path,
                         '/tmp/base_context_properties.csv')
        # filename = '00UQHNO3'
        features_1('/tmp/base_copy.csv', 'datasets', pseudo_gt_model, pseudo_gt_min_max_scaler_path)
        features_2('/tmp/file.csv', 'datasets', '/tmp/base_context.jl', '/tmp/base_context_properties.csv', filename)
        features_3('/tmp/file2.csv', f'Experiments/{experiment_name}/reduced_train_data/{test_dataset}_data',
                   '/tmp/base_graph_embedding_complex.tsv', '/tmp/base_class_count.tsv',
                   '/tmp/base_prop_count.tsv', filename)


handler(None, None)
