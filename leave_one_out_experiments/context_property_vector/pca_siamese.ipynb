{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c70098",
   "metadata": {},
   "source": [
    "## Purpose: \n",
    "\n",
    "For leave one out experiments, context property vector feature, reducing the dimensionality of the properties using PCA\n",
    "\n",
    "For data: Download from AWS S3 - table-linker-datasets/Experiments/context_vector_{train/dev}data into Experiments/ folders\n",
    "Alternatively recreate the data by running the datasets with context_property_vector.py\n",
    "\n",
    "Both forward and reverse properties are considered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1151e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import scipy.sparse as sp\n",
    "import pickle\n",
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b286911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_files = '../Experiments/context_vector_train_data/'\n",
    "all_dev_files = '../Experiments/context_vector_dev_data/'\n",
    "all_properties = ''\n",
    "all_inverse_properties = ''\n",
    "N_COMPONENTS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b0a0502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748\n",
      "211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (29,30,31,36,38,42,43,44) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (42,43,44,49) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (49) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (44,49) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (29,30,36,38) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (29,30,31,36,38) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (36,38) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (29,30,31,36,38,44) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (31,36,38) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (27,28,29,34,36) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (34,36) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (2,6,14,15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (14,15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (44) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_train_files_list = glob.glob(all_train_files + '/*/*.csv')\n",
    "print(len(all_train_files_list))\n",
    "all_dev_files_list = glob.glob(all_dev_files + '/*/*.csv')\n",
    "print(len(all_dev_files_list))\n",
    "\n",
    "properties_set = set()\n",
    "inverse_properties = set()\n",
    "for file in all_train_files_list:\n",
    "    df = pd.read_csv(file)\n",
    "    df['context_property_vector'] = df['context_property_vector'].apply(literal_eval)\n",
    "    property_values = df['context_property_vector'].values\n",
    "    for k in property_values:\n",
    "        for prop in k:\n",
    "            #print(prop)\n",
    "            if '_' in prop:\n",
    "                inverse_properties.add(prop)\n",
    "            else:\n",
    "                properties_set.add(prop)\n",
    "for file in all_dev_files_list:\n",
    "    df = pd.read_csv(file)\n",
    "    df['context_property_vector'] = df['context_property_vector'].apply(literal_eval)\n",
    "    property_values = df['context_property_vector'].values\n",
    "    for k in property_values:\n",
    "        for prop in k:\n",
    "            #print(prop)\n",
    "            if '_' in prop:\n",
    "                inverse_properties.add(prop)\n",
    "            else:\n",
    "                properties_set.add(prop)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "414e8664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4546"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all = properties_set.union(inverse_properties)\n",
    "#len(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6abfe1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(all, open('all_properties.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "227aa660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4546"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all = pickle.load(open('all_properties.pkl', 'rb'))\n",
    "len(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a5b7c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Importing all the libraries\n",
    "import glob\n",
    "import boto3\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from itertools import chain\n",
    "import copy\n",
    "import shutil\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Defining connection to s3\n",
    "\n",
    "###Defining all the results path a\n",
    "features = [\"monge_elkan\",\"monge_elkan_aliases\",\"jaro_winkler\",\n",
    "            \"levenshtein\",\"singleton\", \"num_occurences\"]\n",
    "\n",
    "\n",
    "def read_file(key):\n",
    "    #resp = s3.get_object(Bucket = bucket, Key = key)\n",
    "    try:\n",
    "        df = pd.read_csv(key, sep = ',')\n",
    "    except pd.errors.EmptyDataError:\n",
    "        df = ''\n",
    "        print('Empty csv file!')\n",
    "    return df\n",
    "\n",
    "def save_file(key, content):\n",
    "    s3_res = boto3.resource('s3')\n",
    "\n",
    "    object = s3_res.Object(bucket, key)\n",
    "    result = object.put(Body = content)\n",
    "\n",
    "def merge_files(args):\n",
    "    # datapath = args.train_path\n",
    "    df_list = []\n",
    "    for fn in args.train_files:\n",
    "        fid = fn.split('/')[-1][:-4]\n",
    "        dataset_id = fn.split('/')[-2]\n",
    "        df = read_file(fn)\n",
    "        if not isinstance(df, pd.DataFrame) :\n",
    "            continue\n",
    "\n",
    "        df['table_id'] = fid\n",
    "        df['dataset_id'] = dataset_id\n",
    "        df['context_score'].fillna(0.0, inplace=True)\n",
    "        if 'column-id' not in df.columns:\n",
    "            df['column-id'] = fn.split('/')[-1] + df['column'].astype('str')\n",
    "\n",
    "        df = df[extra_feat]\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "\n",
    "def compute_normalization_factor(args, all_data):\n",
    "    min_max_scaler_path = args.min_max_scaler_path\n",
    "    all_data_features = all_data[features]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(all_data_features)\n",
    "    #pickle.dump(scaler, open('./tmp/min_max_scaler_path.pkl', 'wb'))\n",
    "    #s3_1.Bucket('table-linker-datasets').upload_file('./tmp/min_max_scaler_path.pkl', min_max_scaler_path)\n",
    "\n",
    "    #save_file(min_max_scaler_path, scaler)\n",
    "    return scaler\n",
    "\n",
    "def save_pickle(key, content):\n",
    "    pass\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "def generate_train_data(args, all_data, scaler, shuffle_by = None):\n",
    "    num_cells_1 = 0\n",
    "    scaler_path = args.min_max_scaler_path\n",
    "    #scaler = pickle.load(open('./tmp/min_max_scaler_path.pkl', 'rb'))\n",
    "    final_list = []\n",
    "    sfeatures = copy.deepcopy(features) + ['context_property_vector']\n",
    "    normalize_features = features\n",
    "    evaluation_label = ['evaluation_label']\n",
    "    positive_features_final = []\n",
    "    negative_features_final = []\n",
    "    super_groups = all_data.groupby(['column-id'])\n",
    "    if shuffle_by == 'dataset':\n",
    "        super_groups = all_data.groupby(['dataset_id'])\n",
    "        for i, s_group in super_groups:\n",
    "            pos_features_dataset = []\n",
    "            neg_features_dataset = []\n",
    "            grouped_obj = s_group.groupby(['column', 'row', 'column-id'])\n",
    "\n",
    "            for cell in grouped_obj:\n",
    "                num_cells_1 += 1\n",
    "                cell[1][normalize_features] = scaler.transform(cell[1][normalize_features])\n",
    "                pos_features = []\n",
    "                neg_features = []\n",
    "                a = cell[1][cell[1]['evaluation_label'] == 1]\n",
    "                if a.empty:\n",
    "                    continue\n",
    "                pos_rows = cell[1][cell[1]['evaluation_label'].astype(int) == 1][sfeatures].to_numpy()\n",
    "                if len(pos_rows) < 1:\n",
    "                    continue\n",
    "                if len(pos_rows) > 1:\n",
    "                    print(\"here\")\n",
    "                for i in range(len(pos_rows)):\n",
    "                    pos_features.append(pos_rows[i])\n",
    "                neg_rows = cell[1][cell[1]['evaluation_label'].astype(int) == -1][sfeatures].to_numpy()\n",
    "                for i in range(min(batch_size, len(neg_rows))):\n",
    "                    neg_features.append(neg_rows[i])\n",
    "\n",
    "                for k in range(len(neg_features) - len(pos_features)):\n",
    "                    pos_features.append(random.choice(pos_rows))\n",
    "                if len(pos_features) != len(neg_features):\n",
    "                    continue\n",
    "                pos_features_dataset.append(pos_features)\n",
    "                neg_features_dataset.append(neg_features)\n",
    "            if len(pos_features_dataset) > 0:\n",
    "                c = list(zip(pos_features_dataset, neg_features_dataset))\n",
    "                random.shuffle(c)\n",
    "                pos_features_dataset, neg_features_dataset = zip(*c)\n",
    "                positive_features_final.extend(pos_features_dataset)\n",
    "                negative_features_final.extend(neg_features_dataset)\n",
    "    elif shuffle_by == 'table':\n",
    "        super_groups = all_data.groupby(['table_id'])\n",
    "        for i, s_group in super_groups:\n",
    "            pos_features_table = []\n",
    "            neg_features_table = []\n",
    "            file_name = i.split('-')[0]\n",
    "            #print(\"entering \", file_name)\n",
    "            grouped_obj = s_group.groupby(['column', 'row'])\n",
    "\n",
    "            for cell in grouped_obj:\n",
    "                num_cells_1 += 1\n",
    "                cell[1][normalize_features] = scaler.transform(cell[1][normalize_features])\n",
    "                pos_features = []\n",
    "                neg_features = []\n",
    "                a = cell[1][cell[1]['evaluation_label'] == 1]\n",
    "                if a.empty:\n",
    "                    continue\n",
    "                pos_rows = cell[1][cell[1]['evaluation_label'].astype(int) == 1][sfeatures].to_numpy()\n",
    "                if len(pos_rows) < 1:\n",
    "                    continue\n",
    "                if len(pos_rows) > 1:\n",
    "                    print(\"here\")\n",
    "                for i in range(len(pos_rows)):\n",
    "                    pos_features.append(pos_rows[i])\n",
    "                neg_rows = cell[1][cell[1]['evaluation_label'].astype(int) == -1][sfeatures].to_numpy()\n",
    "                for i in range(min(batch_size, len(neg_rows))):\n",
    "                    neg_features.append(neg_rows[i])\n",
    "\n",
    "                for k in range(len(neg_features) - len(pos_features)):\n",
    "                    pos_features.append(random.choice(pos_rows))\n",
    "                if len(pos_features) != len(neg_features):\n",
    "                    continue\n",
    "                random.shuffle(pos_features)\n",
    "\n",
    "                random.shuffle(neg_features)\n",
    "                pos_features_table.append(pos_features)\n",
    "                neg_features_table.append(neg_features)\n",
    "            if len(pos_features_table) > 0:\n",
    "                c = list(zip(pos_features_table, neg_features_table))\n",
    "                random.shuffle(c)\n",
    "                pos_features_table, neg_features_table = zip(*c)\n",
    "                positive_features_final.extend(pos_features_table)\n",
    "                negative_features_final.extend(neg_features_table)\n",
    "    else:\n",
    "        for i, s_group in super_groups:\n",
    "            file_name = i.split('-')[0]\n",
    "            #print(\"entering \", file_name)\n",
    "            grouped_obj = s_group.groupby(['column', 'row'])\n",
    "            for cell in grouped_obj:\n",
    "                num_cells_1 += 1\n",
    "                cell[1][normalize_features] = scaler.transform(cell[1][normalize_features])\n",
    "                pos_features = []\n",
    "                neg_features = []\n",
    "                a = cell[1][cell[1]['evaluation_label'] == 1]\n",
    "                if a.empty:\n",
    "                    continue\n",
    "                pos_rows = cell[1][cell[1]['evaluation_label'].astype(int) == 1][sfeatures].to_numpy()\n",
    "                if len(pos_rows) < 1:\n",
    "                    continue\n",
    "                if len(pos_rows) > 1:\n",
    "                    print(\"here\")\n",
    "                for i in range(len(pos_rows)):\n",
    "                    pos_features.append(pos_rows[i])\n",
    "                neg_rows = cell[1][cell[1]['evaluation_label'].astype(int) == -1][sfeatures].to_numpy()\n",
    "                for i in range(min(batch_size, len(neg_rows))):\n",
    "                    neg_features.append(neg_rows[i])\n",
    "\n",
    "                for k in range(len(neg_features) - len(pos_features)):\n",
    "                    pos_features.append(random.choice(pos_rows))\n",
    "                random.shuffle(pos_features)\n",
    "                random.shuffle(neg_features)\n",
    "                if len(pos_features) != len(neg_features):\n",
    "                    print(\"HHHERRRERR\")\n",
    "                else:\n",
    "                    positive_features_final.append(pos_features)\n",
    "                    negative_features_final.append(neg_features)\n",
    "                #print(len(positive_features_final), len(positive_features_final[3]))\n",
    "                #print(len(negative_features_final), len(negative_features_final[3]))\n",
    "    if shuffle_by == 'complete_shuffle':\n",
    "        c = list(zip(positive_features_final, negative_features_final))\n",
    "        random.shuffle(c)\n",
    "        positive_features_final, positive_features_final = zip(*c)\n",
    "    print(len(positive_features_final), len(positive_features_final[3]))\n",
    "    print(len(negative_features_final), len(negative_features_final[3]))\n",
    "    pickle.dump(positive_features_final, open('pos_semtab_all_feat_8.pkl', 'wb'))\n",
    "    pickle.dump(negative_features_final, open('neg_semtab_all_feat_8.pkl', 'wb'))\n",
    "    print(len(positive_features_final), len(positive_features_final[3]))\n",
    "    print(len(negative_features_final), len(negative_features_final[3]))\n",
    "\n",
    "\n",
    "def merge_eval_files(final_score_path):\n",
    "\n",
    "    for fn in final_score_path:\n",
    "        fid = fn.split('/')[-1].split('.csv')[0]\n",
    "        df = read_file(fn)\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            continue\n",
    "        df['table_id'] = fid\n",
    "        df_list.append(df)\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "\n",
    "def parse_eval_files_stats(eval_data, method):\n",
    "    res = {}\n",
    "    candidate_eval_data = eval_data.groupby(['table_id', 'column', 'row'])['table_id'].count().reset_index(name=\"count\")\n",
    "    res['num_tasks_with_gt'] = len(eval_data[pd.notna(eval_data['GT_kg_id'])].groupby(['table_id', 'column', 'row']))\n",
    "    num_tasks_with_model_score_top_one_accurate = []\n",
    "    num_tasks_with_model_score_top_five_accurate = []\n",
    "    num_tasks_with_model_score_top_ten_accurate = []\n",
    "    has_gt_list = []\n",
    "    has_gt_in_candidate = []\n",
    "    for i, row in candidate_eval_data.iterrows():\n",
    "        table_id, row_idx, col_idx = row['table_id'], row['row'], row['column']\n",
    "        c_e_data = eval_data[\n",
    "            (eval_data['table_id'] == table_id) & (eval_data['row'] == row_idx) & (eval_data['column'] == col_idx)]\n",
    "        assert len(c_e_data) > 0\n",
    "        if np.nan not in set(c_e_data['GT_kg_id']):\n",
    "            has_gt_list.append(1)\n",
    "        else:\n",
    "            has_gt_list.append(0)\n",
    "        if 1 in set(c_e_data['evaluation_label']):\n",
    "            has_gt_in_candidate.append(1)\n",
    "        else:\n",
    "            has_gt_in_candidate.append(0)\n",
    "\n",
    "        # rank on model score\n",
    "        s_data = c_e_data.sort_values(by=[method], ascending=False)\n",
    "        if s_data.iloc[0]['evaluation_label'] == 1:\n",
    "            num_tasks_with_model_score_top_one_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_one_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:5]['evaluation_label']):\n",
    "            num_tasks_with_model_score_top_five_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_five_accurate.append(0)\n",
    "        if 1 in set(s_data.iloc[0:10]['evaluation_label']):\n",
    "            num_tasks_with_model_score_top_ten_accurate.append(1)\n",
    "        else:\n",
    "            num_tasks_with_model_score_top_ten_accurate.append(0)\n",
    "\n",
    "    res['num_tasks_with_model_score_top_one_accurate'] = sum(num_tasks_with_model_score_top_one_accurate)\n",
    "    res['num_tasks_with_model_score_top_five_accurate'] = sum(num_tasks_with_model_score_top_five_accurate)\n",
    "    res['num_tasks_with_model_score_top_ten_accurate'] = sum(num_tasks_with_model_score_top_ten_accurate)\n",
    "    return res, candidate_eval_data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aef22fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(key):\n",
    "    #resp = s3.get_object(Bucket = bucket, Key = key)\n",
    "    try:\n",
    "        df = pd.read_csv(key, sep = ',')\n",
    "    except pd.errors.EmptyDataError:\n",
    "        df = ''\n",
    "        print('Empty csv file!')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d66b172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "748"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files = []\n",
    "for train_path in train_files_path:\n",
    "    set_of_files = glob.glob(train_path + '/*.csv')\n",
    "    train_files.extend(set_of_files)\n",
    "len(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8c1acfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_files_path = ['../Experiments/context_vector_dev_data/' + i for i in experiment_train_data.split(',')]\n",
    "dev_files = []\n",
    "for dev_path in dev_files_path:\n",
    "    set_of_files = glob.glob(dev_path + '/*.csv')\n",
    "    dev_files.extend(set_of_files)\n",
    "len(dev_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09733d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (29,30,36,38) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (29,30,31,36,38) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (36,38) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (29,30,31,36,38,44) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (31,36,38) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (27,28,29,34,36) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (34,36) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (2,6,14,15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (14,15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (29,30,31,36,38,42,43,44) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (42,43,44,49) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (49) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/nas/home/hrathod/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3418: DtypeWarning: Columns (44,49) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "extra_feat = ['column-id', 'column', 'row', 'evaluation_label', 'dataset_id', 'table_id', 'context_property_vector']\n",
    "for f in features:\n",
    "    extra_feat.append(f)\n",
    "print(\"------------------------------------\")\n",
    "gen_training_data_args = Namespace(train_files=train_files, pos_output=pos_output, neg_output=neg_output,min_max_scaler_path=min_max_scaler_path)\n",
    "print(\"\\n\")\n",
    "scaler = compute_normalization_factor(gen_training_data_args, all_data)\n",
    "print(\"------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(scaler, open('min_max_scaler.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd559334",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_list = list(all)\n",
    "#col_indices_1 = {i:i for i in range(len(features))}\n",
    "col_indices = {properties_list[i]:i for i in range(len(properties_list))}\n",
    "def convert_to_matrix_vector(data: pd.DataFrame, properties_list:list):\n",
    "    #col = properties_list\n",
    "\n",
    "    col_used_up = set()\n",
    "    col = list(range(0, len(properties_list)))\n",
    "    row = list(range(0, len(data)))\n",
    "    \n",
    "    rows, cols, vals = [], [], []\n",
    "    for rows_ind in range(len(data)):\n",
    "         for cols_ind in range(len(data[rows_ind])):\n",
    "                if isinstance(data[rows_ind][cols_ind], str):\n",
    "                    props = literal_eval(data[rows_ind][cols_ind])\n",
    "\n",
    "                    for prop in props:\n",
    "                        rows.append(rows_ind)\n",
    "                        cols.append(col_indices[prop])\n",
    "                        col_used_up.add(cols_ind)\n",
    "                        vals.append(props[prop])\n",
    "\n",
    "    X = sp.csr_matrix((vals, (rows, cols)),  shape=(len(data), len(properties_list))).toarray()\n",
    "    return X\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e740138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47325 8\n",
      "47325 8\n",
      "47325 8\n",
      "47325 8\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "gen_training_data_args = Namespace(train_files=train_files, pos_output=pos_output, neg_output=neg_output,min_max_scaler_path=min_max_scaler_path)\n",
    "generate_train_data(gen_training_data_args, all_data, scaler, shuffle_by = 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3e6b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloader(positive_feat_path, negative_feat_path):\n",
    "    pos_features = pickle.load(open(positive_feat_path, 'rb'))\n",
    "    neg_features = pickle.load(open(negative_feat_path, 'rb'))\n",
    "    print(pos_features[10])\n",
    "    pos_features_flatten = list(chain.from_iterable(pos_features))\n",
    "    neg_features_flatten = list(chain.from_iterable(neg_features))\n",
    "    return pos_features_flatten, neg_features_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d09bca1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.0, 1.0, 1.0, 1.0, 1.0, 0.9463052250822914, 0.9999999999999992,\n",
      "       0.6661640466737692, 1.0, '{}'], dtype=object), array([1.0, 1.0, 1.0, 1.0, 1.0, 0.9463052250822914, 0.9999999999999992,\n",
      "       0.6661640466737692, 1.0, '{}'], dtype=object), array([1.0, 1.0, 1.0, 1.0, 1.0, 0.9463052250822914, 0.9999999999999992,\n",
      "       0.6661640466737692, 1.0, '{}'], dtype=object), array([1.0, 1.0, 1.0, 1.0, 1.0, 0.9463052250822914, 0.9999999999999992,\n",
      "       0.6661640466737692, 1.0, '{}'], dtype=object), array([1.0, 1.0, 1.0, 1.0, 1.0, 0.9463052250822914, 0.9999999999999992,\n",
      "       0.6661640466737692, 1.0, '{}'], dtype=object), array([1.0, 1.0, 1.0, 1.0, 1.0, 0.9463052250822914, 0.9999999999999992,\n",
      "       0.6661640466737692, 1.0, '{}'], dtype=object), array([1.0, 1.0, 1.0, 1.0, 1.0, 0.9463052250822914, 0.9999999999999992,\n",
      "       0.6661640466737692, 1.0, '{}'], dtype=object), array([1.0, 1.0, 1.0, 1.0, 1.0, 0.9463052250822914, 0.9999999999999992,\n",
      "       0.6661640466737692, 1.0, '{}'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "pos, neg = generate_dataloader('pos_semtab_all_feat_8.pkl', 'neg_semtab_all_feat_8.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "701c1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = convert_to_matrix_vector(pos, list(all))\n",
    "Y_data = convert_to_matrix_vector(neg, list(all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "876bf33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378557"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72fe2e2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-7b1c0ea5183b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mval_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val = np.array([[1]]*len(X_data))\n",
    "X_data = np.concatenate((X_data, val), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71126e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = np.array([[0]]*len(Y_data))\n",
    "Y_data = np.concatenate((Y_data, val), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5088c037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit with added parameter to recognize positive and negative\n",
    "pca = IncrementalPCA(n_components = N_COMPONENTS)\n",
    "pca.partial_fit(X_data)\n",
    "pca.partial_fit(Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d39d2fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No added parameter\n",
    "res = np.concatenate((X_data, Y_data), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "192422ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncrementalPCA(n_components=100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = IncrementalPCA(n_components = N_COMPONENTS)\n",
    "pca.fit(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85cc314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pca, open(\"pca_var_100.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
